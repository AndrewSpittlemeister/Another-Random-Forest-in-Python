{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('.venv')",
   "metadata": {
    "interpreter": {
     "hash": "0a11edd73418ac0304368ea2890697e5d158a4fa8829350d01bf2337fc71853d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "source": [
    "## Some Helper Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9182958340544896\n0.4591479170272448\n(array([[ 3, 10],\n       [ 1, 22],\n       [ 2, 28]]), array([[ 5, 32],\n       [ 4, 32]]), array([1, 1, 0]), array([0, 1]))\n"
     ]
    }
   ],
   "source": [
    "def compute_entropy(labels: np.ndarray) -> float:\n",
    "    \"\"\"Computes the entropy of a given set of labels (assumed to be either 0 or 1).\"\"\"\n",
    "\n",
    "    entropy: float = 0.0\n",
    "\n",
    "    # there is zero entropy in an empty set of labels\n",
    "    if labels.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # calculate ratio of true to false labels\n",
    "    true_rate = labels.sum() / labels.shape[0]\n",
    "    false_rate = 1 - true_rate\n",
    "\n",
    "    # return zero entropy if either of these rates are 0\n",
    "    if true_rate == 0 or false_rate == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return -1 * false_rate * np.log2(false_rate) - true_rate * np.log2(true_rate)\n",
    "\n",
    "def compute_information_gain(old_labels: np.ndarray, new_labels_left: np.ndarray, new_labels_right: np.ndarray) -> float:\n",
    "    \"\"\"Computes the information gained from splitting up the labels in old_labels to the two new sets.\"\"\"\n",
    "\n",
    "    information_gain: float = 0.0\n",
    "\n",
    "    # compute old entropy\n",
    "    old_entropy = compute_entropy(old_labels)\n",
    "\n",
    "    # compute new entropy\n",
    "    left_weight = new_labels_left.shape[0] / old_labels.shape[0]\n",
    "    right_weight = new_labels_right.shape[0] / old_labels.shape[0]\n",
    "    new_entropy = (left_weight * compute_entropy(new_labels_left)) + (right_weight * compute_entropy(new_labels_right))\n",
    "\n",
    "    return old_entropy - new_entropy\n",
    "\n",
    "def split_on_attribute(X: np.ndarray, y: np.ndarray, split_attribute_index: int, split_value: int) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Splits the data in X and y according to the chosen split attribute index (a column in X) and\n",
    "        the defined split value.\"\"\"\n",
    "    \n",
    "    X_left = []\n",
    "    X_right = []\n",
    "    y_left = []\n",
    "    y_right = []\n",
    "\n",
    "    for i, feature_vector in enumerate(X):\n",
    "        if feature_vector[split_attribute_index] <= split_value:\n",
    "            X_left.append(feature_vector)\n",
    "            y_left.append(y[i])\n",
    "        else:\n",
    "            X_right.append(feature_vector)\n",
    "            y_right.append(y[i])\n",
    "    \n",
    "    return np.array(X_left), np.array(X_right), np.array(y_left), np.array(y_right)\n",
    "\n",
    "def find_best_split(X: np.ndarray, y: np.ndarray) -> tuple[int, int, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Finds the best way to split up the current dataset for the highest information gain.\"\"\"\n",
    "\n",
    "    N = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "\n",
    "    highest_information_gain = np.NINF\n",
    "    attribute_index = 0\n",
    "    split_value = 0\n",
    "    X_left = np.array([])\n",
    "    X_right = np.array([])\n",
    "    y_left = np.array([])\n",
    "    y_right = np.array([])\n",
    "\n",
    "    # loop through a random choice of attributes\n",
    "    for new_attribute_index in np.random.choice(np.arange(d), int(np.power(d, 0.5)), replace=False).tolist():\n",
    "\n",
    "        # first calculate the value to split on, which will be the mean in this example\n",
    "        new_split_value = X[:, attribute_index].sum() / N\n",
    "\n",
    "        # build partitions\n",
    "        X_left_tmp, X_right_tmp, y_left_tmp, y_right_tmp = split_on_attribute(X, y, new_attribute_index, new_split_value)\n",
    "\n",
    "        # then find the information gain for this split\n",
    "        new_info_gain = compute_information_gain(y, y_left_tmp, y_right_tmp)\n",
    "\n",
    "        # if this is higher than any previous gain, save off the result\n",
    "        if new_info_gain > highest_information_gain:\n",
    "            highest_information_gain = new_info_gain\n",
    "            attribute_index = new_attribute_index\n",
    "            split_value = new_split_value\n",
    "            X_left = X_left_tmp\n",
    "            X_right = X_right_tmp\n",
    "            y_left = y_left_tmp\n",
    "            y_right = y_right_tmp\n",
    "\n",
    "    return attribute_index, split_value, X_left, X_right, y_left, y_right\n",
    "\n",
    "print(compute_entropy(np.array([0,0,0,1,1,1,1,1,1])))\n",
    "print(compute_information_gain(np.array([0,0,0,1,1,1]), np.array([0,0]), np.array([0,1,1,1])))\n",
    "print(split_on_attribute(np.array([[3, 10],[1,22],[2,28],[5,32],[4,32]]), np.array([1,1,0,0,1]), 0, 3))"
   ]
  }
 ]
}